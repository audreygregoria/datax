{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X-RwB75m44SL",
        "outputId": "012f3f85-a3ad-49aa-e6a1-cb3b1e5eab0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "import tweepy\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re \n",
        "import pandas as pd\n",
        "import tweepy \n",
        "from tweepy import OAuthHandler \n",
        "from textblob import TextBlob \n",
        "import csv\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "n_words= set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "porter = PorterStemmer()\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bwYZGQs35nuo"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "api_key = 'ucVkNtONJYEO6dYe8vfYaS9uE'\n",
        "api_key_secret = '2xakPRdl1Xruv7wQJux2h921KD14iS5679Xv5kRCbwZbh4P3Wv'\n",
        "access_token = '1408462079598632963-ZF9gAqaqBhLuz7VYWlMC7AiurzqRH0'\n",
        "access__token_secret = 'YJNu4q3RylxsGVStxcjIGzILxcyWfyrGw5GShV5nx76XT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lUDDjF-e7Vt7"
      },
      "outputs": [],
      "source": [
        "auth_handler = tweepy.OAuthHandler(consumer_key=api_key, consumer_secret=api_key_secret)\n",
        "auth_handler.set_access_token(access_token, access__token_secret)\n",
        "\n",
        "api = tweepy.API(auth_handler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yoeqrcEB-MPX"
      },
      "outputs": [],
      "source": [
        "search_term = 'bored apes'\n",
        "tweet_amount = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HOIJhrdrpTss"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "  # removing @ tags and links from the text\n",
        "  text= ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)\", \" \", text).split()) \n",
        "  # converting all letters to lower case and relacing '-' with spaces.\n",
        "  text= text.lower().replace('-', ' ')\n",
        "  # removing stowards and numbers\n",
        "  table= str.maketrans('', '', string.punctuation+string.digits)\n",
        "  text= text.translate(table)\n",
        "  # tokenizing words\n",
        "  tokens = word_tokenize(text)\n",
        "  # stemming the words \n",
        "  stemmed = [porter.stem(word) for word in tokens]\n",
        "  words = [w for w in stemmed if not w in n_words]\n",
        "  # lemmetizing words\n",
        "  # lemme= [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  # words = [w for w in lemme if not w in n_words]\n",
        "  text = ' '.join(words)\n",
        "  return text "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wordcloud_draw(data, color = 'black'):\n",
        "    words = ' '.join(data)\n",
        "    cleaned_word = \" \".join([word for word in words.split()\n",
        "                            if 'http' not in word  # double check for nay links\n",
        "                                and not word.startswith('#')  # removing hash tags\n",
        "                                and word != 'rt'  \n",
        "                            ])\n",
        "    wordcloud = WordCloud(stopwords=STOPWORDS, # using stopwords provided by Word cloud its optional since we already removed stopwords :)\n",
        "                      background_color=color,\n",
        "                      width=2500,\n",
        "                      height=2000\n",
        "                     ).generate(cleaned_word)\n",
        "    # using matplotlib to display the images in notebook itself.\n",
        "    plt.figure(1,figsize=(13, 13))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "9wZTEEBacg93"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output(search_term): \n",
        "  tweet_amount = 1000\n",
        "  tweets = tweepy.Cursor(api.search, q= search_term, lang = 'en',wait_on_rate_limit=True).items(tweet_amount)\n",
        "  tweetss=[]\n",
        "  for tweet in tweets:\n",
        "      tweet= clean(tweet.text)\n",
        "      analysis = TextBlob(tweet)\n",
        "      senti= analysis.sentiment.polarity\n",
        "      if senti<0 :\n",
        "        emotion = \"NEG\"\n",
        "      elif senti>0:\n",
        "        emotion= \"POS\"\n",
        "      else:\n",
        "        emotion= \"NEU\"\n",
        "      tweetss.append((tweet, senti, emotion))\n",
        "  df= pd.DataFrame(tweetss, columns= ['tweets', 'senti', 'emotion'])\n",
        "  # droping retweets\n",
        "  df= df.drop_duplicates()\n",
        "  # saving to CSV file\n",
        "  df.to_csv('data.csv', index= False)\n",
        "  df_neu= df[ df['emotion'] == 'NEU']\n",
        "  df_neu= df_neu['tweets']\n",
        "  df_pos = df[ df['emotion'] == 'POS']\n",
        "  df_pos = df_pos['tweets']\n",
        "  df_neg = df[ df['emotion'] == 'NEG']\n",
        "  df_neg = df_neg['tweets']\n",
        "  ratio_pos= len(df_pos)/len(df)\n",
        "  ratio_neg= len(df_neg)/len(df)\n",
        "  ratio_neu= len(df_neu)/len(df)\n",
        "\n",
        "  percent_pos = ratio_pos * 100\n",
        "  percent_neg = ratio_neg * 100 \n",
        "  percent_neu = ratio_neu * 100 \n",
        "  return [percent_pos, percent_neg, percent_neu]"
      ],
      "metadata": {
        "id": "2B25k3N8a-sQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anvil-uplink"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "UHivM_OHrJ6O",
        "outputId": "13e66db1-a35c-4e72-8b55-c0ca328b61a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anvil-uplink in /usr/local/lib/python3.7/dist-packages (0.3.42)\n",
            "Requirement already satisfied: ws4py in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from anvil-uplink) (1.15.0)\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anvil.server\n",
        "anvil.server.connect('server_57AQBXHXBXQB3SV63LDEGSMA-UP4OJMY33G3H35TG')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPgNiicik7_h",
        "outputId": "90f73d63-bf68-40dd-9fc0-881838f129dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Published\" as SERVER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@anvil.server.callable\n",
        "def pos(search_term): \n",
        "  pos_result = output(search_term)\n",
        "  return pos_result[0]"
      ],
      "metadata": {
        "id": "I9yo_en_rcXB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oHh3Twtzqapv",
        "outputId": "b6cf43e0-eca8-41b5-9016-f9241a5e813d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34.21052631578947"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "pos('try')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@anvil.server.callable\n",
        "def neg(search_term): \n",
        "  neg_result = output(search_term)\n",
        "  return neg_result[1]"
      ],
      "metadata": {
        "id": "I7EW-sm5tPId"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@anvil.server.callable\n",
        "def neu(search_term): \n",
        "  neu_result = output(search_term)\n",
        "  return neu_result[2]"
      ],
      "metadata": {
        "id": "FjDD5ozstqOX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S0fU-hbkttb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "NFT_Tweets_sentiment_analysis .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}